---
title: "Examen 3A"
author: "Cruz Martínez Ricardo y Vasquez Arriaga Jorge"
date: ""
header-includes:
   - \usepackage[spanish]{babel}
   - \usepackage[utf8]{inputenc}
   - \decimalpoint
   - \usepackage{booktabs}
   - \usepackage{longtable}
   - \usepackage{array}
   - \usepackage{multirow}
   - \usepackage{wrapfig}
   - \usepackage{float}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{tabu}
   - \usepackage{threeparttable}
   - \usepackage{threeparttablex}
   - \usepackage[normalem]{ulem}
   - \usepackage{makecell}
   - \usepackage{xcolor}
   - \usepackage{fancyhdr}
   - \usepackage{lastpage}
output:
  pdf_document: 
    keep_tex: yes
  html_document:
    df_print: paged
urlcolor: blue
---
```{r setup, include=FALSE}
rm(list = ls(all.names = TRUE))
gc()
#tinytex::install_tinytex()
library(reticulate)
knitr::opts_chunk$set(echo = F, warning = F, message = F, error = F, fig.height = 4, fig.width = 8)
library(xtable)
library(knitr)
library(tidyverse)
library(latex2exp)
options(kableExtra.latex.load_packages = FALSE)
library(kableExtra)
```

## 1 Muestreo bietápico

 Un estudiante de posgrado tiene una colección  de 400 libros en su librero. Para estimar  el número total de palabras en su colección de libros,  selecciona una muestra $s_I$ de dos libros usando un muestreo aleatorio simple sin reemplazo. Para los dos libros seleccionados tiene el número de páginas de cada uno. Posteriormente, en cada libro seleccionado selecciona dos páginas usando un muestreo aleatorio simple sin reemplazo. Una vez seleccionada la página registra de cada una el número de palabras que contiene. La información muestral es como sigue.
 
 \begin{table}[H]
  \centering
  \caption{Muestras seleccionadas en cada etapa y valores observados}
    \begin{tabular}{cccc}\\
    Libro  & Número &Páginas & Número de Palabras   \\
    seleccionado ($s_I$) &de páginas ($N_i$)  & seleccionadas ($s_i$) & en la página ($y_k$) \\ \hline
    195   & 300   & 61    &  23    \\
         &       & 212   &  25    \vspace{.2cm}\\
    288   & 200   & 99    & 20    \\
          &       & 111   & 20   \\ \hline
   \end{tabular}
\end{table}

Aquí las páginas conforman la población de interés y los libros son las upm. 


\textbf{i)}  Calcule las probabilidades de inclusión de primer y segundo orden correspondientes a las páginas seleccionadas. 

Por el \textbf{Resultado 5.4} de las notas, tenemos que las probabilidades de primer orden, cosiderando el muestreo bietápico están dadas por
$$\pi_k = \pi_{k|i}\pi_{Ii},$$
donde $\pi_{Ii}$ representa lal probabilidad de inclusión de la $upm$ $i$ (en este caso, serían los libros) y $\pi_{k|i}$ la probabilidad de inclusión del elemento $k$ cuando se realiza la selección en la $upm$ $i$ (sería la probabilidad de escoger una página del libro $i$) y dado que en la primera y segunda 'etapa' se usó un muestreo aleatorio simple sin reemplazo, se tiene que las probabilidades de inclusión correspondientes a las páginas seleccionadas son:
$$\pi_{61} = \frac{2}{400} \cdot \frac{2}{300} = \frac{1}{30000} = \pi_{212}$$
y 
$$\pi_{99}  = \frac{2}{400} \cdot \frac{2}{200} = \frac{1}{20000} = \pi_{111},$$
pues
$$\pi_{I_{195}} = \frac{2}{400} = \pi_{I_{288}},$$
además
$$\pi_{61|195} = \frac{2}{300} = \pi_{212|195} \quad y \quad \pi_{99|288} = \frac{2}{200} = \pi_{111|288}.$$
Por otra parte, sabemos que las probabilidades de incluisión de segundo orden están dadas por

\[ \pi_{kl}=\begin{cases} 
      \pi_{Ii}\pi_{kl|i} & \text{si } i=j \\
      \pi_{Iij}\pi_{k|i}\pi_{l|j} & \text{si } i\neq j  
   \end{cases}
\]

donde $\pi_{Iij}$ representa la probabilidad de inclusión de segundo orden para las $upm$ $i$ y $j$, $i\neq j$ y $\pi_{kl|i}$ la probablidad de inclusión de segundo orden de los elementos $k$ y $l$ cuando se realiza la selección en la $upm$ $i$, entonces, se tiene que
$$\pi_{61 \& 212} = \pi_{I_{195}}\pi_{61\&212 | 195} = \frac{2}{400}\cdot\frac{2}{300(299)} = \frac{1}{8970000}$$
$$\pi_{99\&111} = \pi_{I_{288}}\pi_{99\&111 | 288} = \frac{2}{400}\cdot \frac{2}{200(199)}= \frac{1}{3980000} $$
$$\pi_{61 \& 99} = \pi_{I_{195 \&288}}\pi_{61|195}\pi_{99|288} = \frac{2}{400(399)}\cdot\frac{2}{300}\cdot\frac{2}{200} = 8.354218881\times 10^{-10}$$
$$\pi_{61 \& 111} = \pi_{I_{195 \&288}}\pi_{61|195}\pi_{111|288} = \frac{2}{400(399)}\cdot\frac{2}{300}\cdot\frac{2}{200} = 8.354218881\times 10^{-10}$$
$$\pi_{212\&99} = \pi_{I_{195 \&288}} \pi_{212|195}\pi_{99|288} = \frac{2}{400(399)}\cdot\frac{2}{300} \cdot\frac{2}{200} = 8.354218881\times 10^{-10} $$
$$\pi_{212\&111} = \pi_{I_{195 \&288}} \pi_{212|195}\pi_{111|288} = \frac{2}{400(399)}\cdot\frac{2}{300}\cdot\frac{2}{200} = 8.354218881\times 10^{-10}$$
\textbf{ii)} Calcule el valor estimado del número total de palabras en la colección de libros.

Por el \textbf{Resultado 5.5} el estimador HT para el total se ve como
$$\widehat{t}_{y\pi} = \sum_{i\in s_I}\frac{\widehat{t}_{i\pi}}{\pi_{Ii}}$$
con $\widehat{t}_{i\pi} = \sum_{k\in s_i}\frac{y_k}{\pi_{k|i}}$, por ende, tenemos que la estimación quedaría de la siguiente manera:
\begin{enumerate}
    \item Para $S_I = 195$, se tiene que
    $$ \widehat{t}_{195\pi} = \sum_{k \in \{61, 212\}}\frac{300}{2}y_k = 150(23 + 25) = 7200$$
    \item Para $S_I = 288$, se tiene
    $$ \widehat{t}_{288\pi} = \sum_{k \in \{99, 111\}}\frac{200}{2}y_k = 100(40) = 4000 $$
\end{enumerate}
Por último, nuestra estimación del total quedaría como:
$$ \widehat{t}_{y\pi} = \sum_{i \in \{195, 288\}}200 \widehat{t}_{i\pi} = 200(7200 + 4000) = 2240000$$
\textbf{iii)} Dé una estimación insesgada de la varianza del estimador usado en ii).

Sabemos, por el \textbf{Resultado 5.6}, que un estimador insesgado de la varianza del estimador del inciso anterior está dado por
$$\widehat{V}(\widehat{t}_{y\pi}) = \widehat{V}_{PSU} + \widehat{V}_{SSU} = \sum_{i\in s_I}\sum_{j\in s_I}\widehat{\Delta}_{Iij}\frac{\widehat{t}_{i\pi}}{\pi_{Ii}}\frac{\widehat{t}_{j\pi}}{\pi_{Ij}} + \sum_{i\in s_I}\frac{\widehat{V}_i}{\pi_{Ii}},$$

donde $\widehat{\Delta}_{Iij} = \frac{\pi_{Iij} - \pi_{Ii}\pi_{Ij}}{\pi_{Iij}}$ y $\widehat{V}_i= \sum_{k\in s_i}\sum_{l\in s_i} \widehat{\Delta}_{kl|i}\frac{y_k}{\pi_{k|i}}\frac{y_l}{\pi_{l|i}}$, con $\widehat{\Delta}_{kl|i} = \frac{\pi_{kl|i} - \pi_{k|i}\pi_{l|i}}{\pi_{kl|i}}$, dado que en la primera y segunda etapa se usó un muestreo aleatorio simple sin reemplazo, tenemos que 
$$\sum_{i\in s_I}\sum_{j\in s_I}\widehat{\Delta}_{Iij}\frac{\widehat{t}_{i\pi}}{\pi_{Ii}}\frac{\widehat{t}_{j\pi}}{\pi_{Ij}}$$
es la expresión de la varianza del estimador HT para un muestreo aleatorio simple sin reemplazo para la primera estapa, por lo que se tiene que
$$\sum_{i\in s_I}\sum_{j\in s_I}\widehat{\Delta}_{Iij}\frac{\widehat{t}_{i\pi}}{\pi_{Ii}}\frac{\widehat{t}_{j\pi}}{\pi_{Ij}} = \frac{N^2}{n}\left( 1 - \frac{n}{N} \right)S^2_{\widehat{t_i}_\pi s_I}$$
con $S^2_{\widehat{t_i}_\pi s_I} = \frac{\sum_{k\in s_I}(\widehat{t}_{i\pi} - \overline{\widehat{t}_{\pi}})^2}{n-1}$,  $\overline{\widehat{t}_{\pi}} = \frac{\sum_{i\in s_I}\widehat{t}_{i\pi}}{n}$, $N = 400$ y $n = 2$.

```{r, echo=TRUE, cache=TRUE}
s_I <-c(195,195, 288, 288)
N_i <-c(300,300, 200, 200)
s_i <-c(61, 212, 99, 111)
y_k <-c(23, 25, 20,20)

Datos <- data.frame(s_I, N_i, s_i, y_k)

t_i = c(7200, 4000)
 
PSU = var(t_i)*(400^2 / 2) * (1 - 2/400)
PSU
```
De manera similar, tenemos que el segundo término del sumando, dado que a cada $V_i$ la podemos pensar como una suma de varianzas del estimador HT aplicado a las páginas de los libros seleccionados, se tiene que
$$\widehat{V}_i= \sum_{k\in s_i}\sum_{l\in s_i} \widehat{\Delta}_{kl|i}\frac{y_k}{\pi_{k|i}}\frac{y_l}{\pi_{l|i}} = \frac{N_i^2}{2}\left( 1 - \frac{2}{N_i} \right) S^2_{y s_i}$$
con $S^2_{y s_i} = \frac{\sum_{k\in s_i}(y_k - \overline{y}_{s_i})^2}{n-1}$ y $\overline{y}_{s_i} = \frac{\sum_{k\in s_i}y_k}{n}$
```{r}
library(dplyr)
```


```{r, echo=TRUE}
library(dplyr)
V_i = Datos %>% group_by(s_I)%>%summarise(V_i =unique(N_i)^2/2*(1

     - 2/unique(N_i))*var(y_k))%>%ungroup()

SSU = sum(V_i) * (400 / 2)

SSU
```

Finalmente, la estimación de la varianza estaría dada por

```{r, echo=TRUE}
PSU + SSU
```


\textbf{iv)} Dé una aproximación de la varianza del estimador usado en ii) que sólo use las probabilidades de inclusión de primer orden o los factores de expansión.

Sabemos, por la ecuación $(168)$ de las notas, que bajo los supuestos de $\pi_{Ii} = n_I p_i$ y $m_I = n_I$ usando un muestreo con reemplazo para seleccionar las upm, se tiene que una estimación de la varianza es la siguiente
$$ \widehat{V}(\widehat{t}_{y\pi}) = \frac{n_I}{n_I - 1}\sum_{i = 1}^{n_I}\left(\frac{\widehat{t}_{i\pi}}{\pi_{Ii}} - \frac{\widehat{t}_{y\pi}}{n_I} \right)^2$$
en nuestro caso, tenemos que $n_I = 2$, $\frac{\widehat{t}_{y\pi}}{2} = \frac{\frac{400}{2} \cdot (7200 + 4000)}{2} = \frac{2240000}{2} = 1120000$, por ende, tenemos lo siguiente

```{r, echo=TRUE}
t_ypi2 = 1120000
estVar <- 2 * sum((400/2 * t_i - t_ypi2)^2)
estVar
```



## 2 Comparación de diseños de muestreo

Suponga que se realizará una nueva elección de diputaciones a nivel federal y le han encargado realizar el diseño de muestreo. Para esto cuenta con la información de los resultados a nivel acta (casilla) de los cómputos distritales de 2021 (https://computos2021.ine.mx/base-de-datos). 

Por simplicidad suponga que se centrará en estudiar los diseños de manera que elegirá el que sirva para realizar de la mejor forma la estimación del porcentaje de votos a favor de una coalición integrada por Morena, PT y P. Verde a nivel nacional.

En el denominador considerará sólo el total de votos válidos, es decir, que no se considerarán los votos nulos para efectos del cálculo del porcentaje.

Consideré los siguientes tres diseños a comparar:

i.    Se seleccionan 1200 casillas de las 163,666 usando un muestreo aleatorio simple sin reemplazo.
ii.   Se considera una estratificación a partir de los 300 diferentes distritos electorales del país. El diseño de muestreo considerado en cada estrato corresponde a un muestreo aleatorio simple sin reemplazo de 4 casillas.
iii.  Se considera una estratificación considerando las cinco circunscripciones electorales de México. En cada estrato se usa un muestreo aleatorio simple sin reemplazo para seleccionar 8 distritos electorales y en cada distrito electoral seleccionado se usa un muestreo aleatorio simple para seleccionar 30 casillas.

Realice la comparación de los tres diseños a partir de simulaciones. Es decir, repita 1000 veces lo siguiente. Seleccione una muestra con cada diseño y realice la estimación, en este caso deberá usar un estimador de razón. 

A partir de las 1000 estimaciones estime el ECM del estimador que se obtendría para cada diseño y con estos resultados indique cuál diseño parece ser el mejor.

Para resolver este problema optamos por resolver manualmente debido a que con sampling(strata, stage) y survey, se tardaba demasiado tiempo.

```{r,echo=TRUE}
# libreria para limpiar la base de datos
library(tidyverse)

# leer el csv
setwd("D:/Notas/Muestreo/Exámen/examen02/20210611_1000_CW_diputaciones")
options(digits=10)
set.seed(123)


df <- data.table::fread("diputaciones - copia.csv", sep="|")
# para el INE 34. TOTAL_VOTOS_CALCULADOS -  Suma de los votos asentados en las
# actas para: los partidos políticos, combinaciones de estos, candidatos 
# independientes, votos para candidaturas no registradas y votos nulos. 

# como solo nos interesa el porcentaje sobre la gente que no registro voto nulo
# entonces los restamos
# y solo nos interesa las casillas donde si hubo votaciones
df <- filter(df, TOTAL_VOTOS_CALCULADOS != 0)
df <- df %>%drop_na(TOTAL_VOTOS_CALCULADOS)
df <- df %>%drop_na(`VOTOS NULOS`)
df$totalvotos = df$TOTAL_VOTOS_CALCULADOS - df$`VOTOS NULOS`
df <- filter(df, totalvotos != 0)

# el número total de casilla sobre el que vamos a trabajr
N <- dim(df)[1]
# tamaño de muestra
n <- 1200

# datos que nos interesan
midf <- df[, c("MORENA", "PVEM", "PT", "PT-MORENA",
               "PVEM-PT", "PVEM-MORENA", "PVEM-PT-MORENA", "totalvotos")]
# calculamos los votos a favor
midf$votos.favor = apply(midf,1, function(x) (sum(x)- x[length(x)]))

# resultado a estimar
prct.votos <- sum(midf$votos.favor)/sum(midf$totalvotos)*100
prct.votos

# pesos para el mas
midf$wk.mas = N/n


diseno.mas <- function(){
  #seleccionamos muestra
  seleccion_muestra <- sample(1:N ,size = 1200)
  
  #estimamos con el estimador de razon de la media
  muestra <- midf[seleccion_muestra, ]
  return(sum(muestra$votos.favor*muestra$wk.mas)/sum(muestra$totalvotos*muestra$wk.mas))
}

simulacion.mas <- replicate(1000, diseno.mas())
mean(simulacion.mas)*100
```

```{r, echo=T, cache=TRUE}
# #estratos
df$distritoelec <- paste(df$ID_ESTADO, df$ID_DISTRITO, df$NOMBRE_DISTRITO, sep="-")
# revisamos el tamaño deben ser  300 unicos
length(unique(df$distritoelec))
midf2 <- df[, c("distritoelec", "totalvotos")]
midf2$votos.favor <- midf$votos.favor

# otorgamos id a cada distrito
iddistrito = 1:300
distritoelec = unique(df$distritoelec)
idsdist <- data.frame(iddistrito, distritoelec)
midf2 <- merge(midf2, idsdist)

# calculo de las probabilidades de inclusión para todos los elementos
prob.inc <- data.frame(midf2 %>% group_by(iddistrito) %>% count())
prob.inc$wk <- 1/(4/prob.inc$n)

midf2 <- merge(midf2, prob.inc, by="iddistrito")

# creamos un subid (id dentro del distrito electoral)
subid <- c()
for (i in 1:300){
  s0 = midf2$distritoelec[midf2$iddistrito == i]
  subid = c(subid, 1:length(s0))
}
midf2$subid <- subid

# creamos un id único en toda la  ppoblacion
midf2$idun <- paste(midf2$iddistrito, midf2$subid, sep = "-")


# observemos que esta es la función que se repitara 1000 vveces para la simulación
# lo que reduce el tiempo de ejecución
disenost <- function(){
  #seleccionamos muestra con base en el iduniico
  idun <- c()
  for (i in 1:300) {
    idun <- c(idun, paste(i, sample(1:prob.inc$n[i], size = 4), sep="-"))
  }
  seleccion <- data.frame(idun)
  #obtenemos los datos de la muestra
  muestra <- left_join(seleccion, midf2, by = "idun")
  
  #esimaciones
  return(sum(muestra$votos.favor*muestra$wk)/sum(muestra$totalvotos*muestra$wk))
}
simulacion.estr <- replicate(1000, disenost())
mean(simulacion.estr)*100
```

```{r, echo=T, cache=TRUE}
# trabjaremos con un dataframe con la siguiente
midf3 <- df[, c("NOMBRE_ESTADO", "NOMBRE_DISTRITO", "ID_DISTRITO", "ID_ESTADO",
                "distritoelec", "totalvotos")]
# recuperamos las estimaciones hechas anteriormente 
midf3$votos.favor <- midf$votos.favor

# asignamos circunscripciones con respecto a los estados
circ <- c(2, 1, 1, 3, 2, 5, 3, 1, 4, 1, 2, 4, 5, 1, 5, 5, 4, 1, 2, 3, 4, 
          2, 3, 2, 1, 1, 3, 2, 4, 3, 3, 2)
# recuperamos los nombres de los estados y hacemos un dataframe con los estados
# y circunscripciones
NOMBRE_ESTADO <- unique(df$NOMBRE_ESTADO)
circdf <- data.frame(circ, NOMBRE_ESTADO)
# unimos los dataframes y ahora tenemos la circunscripcion en el dataframe
midf3 <- merge(circdf, midf3, by="NOMBRE_ESTADO")


# otorgamos id unico a cada distrito
iddistrito = 1:300
distritoelec = unique(df$distritoelec)
idsdist <- data.frame(iddistrito, distritoelec)
midf3 <- merge(midf3, idsdist)

#agrupara cada estrato (hacemos un dataframe para cada circunscripción)
circ1 <- midf3 %>% filter(circ == 1)
circ2 <- midf3 %>% filter(circ == 2)
circ3 <- midf3 %>% filter(circ == 3)
circ4 <- midf3 %>% filter(circ == 4)
circ5 <- midf3 %>% filter(circ == 5)

# ahora a cada circunscripción le calcularemos cuantas casillas casillas le 
# pertencecen, tambien un sub-id (que sera un id dentro de cada distrito), 
# vector que guardara los subids
subid <- c()
# vector que guardara cuantas casillas le pertencen a cada distrito
npob <- c()
for (i in unique(circ1$iddistrito)){
  # selecionamos distritos únicos por cirscunscripción 
  s0 = circ1$distritoelec[circ1$iddistrito == i]
  # agregamos el subid
  subid = c(subid, 1:length(s0))
  # total de casillas que le pertencen al distrito
  npob <- c(npob, rep(length(s0), length(s0)))
}
# agregamos los resultados anteriores al dataframe
circ1$subid <- subid
circ1$npob <- npob
# calculamos los pesos para cada casilla
circ1$wk <- 1/(8/length(unique(circ1$iddistrito))*30/circ1$npob)
# creamos un idunico para cada casilla con base en iddistrito y subid
circ1$idun <- paste(circ1$iddistrito, circ1$subid, sep="-")
# relaciona el numero de distrito con el numero de casillas
prob.c1 <- data.frame(circ1 %>% group_by(iddistrito) %>% count())
#repetimos este proceso para cada circunscripción

#circunscripción 2
subid <- c()
npob <- c()
for (i in unique(circ2$iddistrito)){
  s0 = circ2$distritoelec[circ2$iddistrito == i]
  subid = c(subid, 1:length(s0))
  npob <- c(npob, rep(length(s0), length(s0)))
}
circ2$subid <- subid
circ2$npob <- npob
circ2$wk <- 1/(8/length(unique(circ2$iddistrito))*30/circ2$npob)
prob.c2 <- data.frame(circ2 %>% group_by(iddistrito) %>% count())
circ2$idun <- paste(circ2$iddistrito, circ2$subid, sep="-")

# circunscripción 3
subid <- c()
npob <- c()
for (i in unique(circ3$iddistrito)){
  s0 = circ3$distritoelec[circ3$iddistrito == i]
  subid = c(subid, 1:length(s0))
  npob <- c(npob, rep(length(s0), length(s0)))
}
circ3$subid <- subid
circ3$npob <- npob
circ3$wk <- 1/(8/length(unique(circ3$iddistrito))*30/circ3$npob)
prob.c3 <- data.frame(circ3 %>% group_by(iddistrito) %>% count())
circ3$idun <- paste(circ3$iddistrito, circ3$subid, sep="-")

# circunscripcion 4
subid <- c()
npob <- c()
for (i in unique(circ4$iddistrito)){
  s0 = circ4$distritoelec[circ4$iddistrito == i]
  subid = c(subid, 1:length(s0))
  npob <- c(npob, rep(length(s0), length(s0)))
}
circ4$subid <- subid
circ4$npob <- npob
circ4$wk <- 1/(8/length(unique(circ4$iddistrito))*30/circ4$npob)
prob.c4 <- data.frame(circ4 %>% group_by(iddistrito) %>% count())
circ4$idun <- paste(circ4$iddistrito, circ4$subid, sep="-")

# circunscripcion 5
subid <- c()
npob <- c()
for (i in unique(circ5$iddistrito)){
  s0 = circ5$distritoelec[circ5$iddistrito == i]
  subid = c(subid, 1:length(s0))
  npob <- c(npob, rep(length(s0), length(s0)))
}
circ5$subid <- subid
circ5$npob <- npob
circ5$wk <- 1/(8/length(unique(circ5$iddistrito))*30/circ5$npob)
prob.c5 <- data.frame(circ5 %>% group_by(iddistrito) %>% count())
circ5$idun <- paste(circ5$iddistrito, circ5$subid, sep="-")



diseno.est.c1 <- function(){
  # seleccionamos 8 distritos de la circunscripción
  seleccion <-  sample(unique(circ1$iddistrito), size=8)
  # aquí guardaremos el idunico de las casillas seleccions en la muestra
  idun <- c()
  # para cada distrito de los 8 seleccionados
  for (i in seleccion) {
    # vemos cuantas casillas le pertenecen
    iddist <- prob.c1 %>% filter(iddistrito == i)
    # y seleccionamos 30 elemento con un m.a.s.
    idun <- c(idun, paste(i, sample(iddist$n, size = 30), sep="-"))
  }
  #obtenemos los datos de la muestra
  muestra.c1 <- left_join(data.frame(idun), circ1, by = "idun")
  # recuperamos la muestra de esta circunscripción
  return(muestra.c1)
}
# de manera analoga para el resto de circunscripciones

#circunscripcion 2
diseno.est.c2 <- function(){
  seleccion <-  sample(unique(circ2$iddistrito), size=8)
  idun <- c()
  for (i in seleccion) {
    iddist <- prob.c2 %>% filter(iddistrito == i)
    idun <- c(idun, paste(i, sample(iddist$n, size = 30), sep="-"))
  }
  muestra.c2 <- left_join(data.frame(idun), circ2, by = "idun")
  
  return(muestra.c2)
}

#circunscrición 3
diseno.est.c3 <- function(){
  seleccion <-  sample(unique(circ3$iddistrito), size=8)
  idun <- c()
  for (i in seleccion) {
    iddist <- prob.c3 %>% filter(iddistrito == i)
    idun <- c(idun, paste(i, sample(iddist$n, size = 30), sep="-"))
  }
  muestra.c3 <- left_join(data.frame(idun), circ3, by = "idun")
  
  return(muestra.c3)
}

#circunscripción 4
diseno.est.c4 <- function(){
  seleccion <-  sample(unique(circ4$iddistrito), size=8)
  idun <- c()
  for (i in seleccion) {
    iddist <- prob.c4 %>% filter(iddistrito == i)
    idun <- c(idun, paste(i, sample(iddist$n, size = 30), sep="-"))
  }
  muestra.c4 <- left_join(data.frame(idun), circ4, by = "idun")
  
  return(muestra.c4)
}


#circunscripción 5
diseno.est.c5 <- function(){
  seleccion <-  sample(unique(circ5$iddistrito), size=8)
  idun <- c()
  for (i in seleccion) {
    iddist <- prob.c5 %>% filter(iddistrito == i)
    idun <- c(idun, paste(i, sample(iddist$n, size = 30), sep="-"))
  }
  #obtenemos los datos de la muestra
  muestra.c5 <- left_join(data.frame(idun), circ5, by = "idun")
  
  return(muestra.c5)
}

diseno.circs <- function(){
  c1 <- diseno.est.c1()
  c2 <- diseno.est.c2()
  c3 <- diseno.est.c3()
  c4 <- diseno.est.c4()
  c5 <- diseno.est.c5()
  
  muestra.circs <- union(c1,c2)
  muestra.circs <- union(muestra.circs,c3)
  muestra.circs <- union(muestra.circs,c4)
  muestra.circs <- union(muestra.circs,c5)
  
  return(sum(muestra.circs$votos.favor*muestra.circs$wk)/sum(muestra.circs$totalvotos*muestra.circs$wk))
}

simulacion.circs <- replicate(1000, diseno.circs())
mean(simulacion.circs)*100
```
```{r, echo=TRUE}
# ahora calculamos los errores cuadráticos medios de cada diseño

# del diseño 1
mean((100*simulacion.mas - prct.votos)^2)

# del diseño 2
mean((100*simulacion.estr - prct.votos)^2)

# del diseño 3
mean((100*simulacion.circs - prct.votos)^2)
```


```{r,echo=TRUE, cache=TRUE}
hist(100*simulacion.mas, freq = F, breaks = 50, 
     main = "Simulación muestro aleatorio simple",
     xlab = "Porcentaje de votos a favor", ylab = "Frecuencia relativa")
abline(v = prct.votos, add = T, col=2)
hist(100*simulacion.estr, freq = F, breaks = 50, 
     main = "Simulación muestro estratificado",
     xlab = "Porcentaje votos a favor", ylab="Frecuencia relativa")
abline(v = prct.votos, add = T, col=2)
hist(100*simulacion.circs, freq = F, breaks = 50,
     main = "Simulacion muestreo estratificado y bietápico",
     xlab = "Porcentaje votos a favor", ylab = "Frecuencia relativa")
abline(v = prct.votos, add = T, col=2)
```
De lo anterior vemos que el diseño que mejor funciona es el diseño dos, esto es porque a cada distrito le estamos tomando muestra, a diferencia del diseño tres donde solo le tomamos muestra a 40 de 300 distritos, esto hace que sea el peor, sin embargo en cuestiones de costo, puede llegar a ser el más económico, finalmente el diseño que esta en medio es el aleatorio simple.

## 3 Estimación básica de una encuesta con diseño multietápico

Considere la Encuesta Nacional de Vivienda (ENVI) 2020

https://www.inegi.org.mx/programas/envi/2020/ 

Suponga que será el encargado de generar los resultados básicos presentados en el tabulado llamado Cuadro 5.1, ver Figura \ref{Envi2020} (https://www.inegi.org.mx/contenidos/programas/envi/2020/tabulados/envi_2020_tema_05_xlsx.zip)


En particular realice lo siguiente

i.    Describa brevemente el diseño de muestreo usado en la encuesta. Es decir, si es muestreo aleatorio simple, tiene estratificación, es por conglomerados, etc.

Se usó un diseño probabilístico, cuyo diseño muestral fue bietápico, donde en la primera etapa se usó un diseño de muestreo estratificado y en la segunda etapa un diseño de muestreo por conglomerados.

i.    Identifique las variables asociadas al diseño de muestreo que están presentes en la base de datos a usar (THOGAR en https://www.inegi.org.mx/programas/envi/2020/#Microdatos)
i.    Identifique la pregunta y variable asociada a la identificación de los Hogares con necesidad de rentar, comprar o construir una vivienda independiente de la que habitan.
i.    Con esta información, estime el número total de hogares y el porcentaje de hogares que tienen una necesidad de vivienda a nivel nacional y por entidad federativa. 
i.    Calcule intervalos de confianza para los parámetros estimados en el inciso anterior. Comente sobre los resultados obtenidos.
i.    **Punto extra opcional**. Considerando el porcentaje de hogares que tienen una necesidad de vivienda por entidad federativa, realice un mapa de calor (Geographic Heat Map) y comente los resultados. 

\begin{figure}[H]
\centering
	\includegraphics[width=160mm]{images/ENVI2020.png}
	\caption{Parte del cuadro 5.1 de los resultados de la ENVI 2020}
	\label{Envi2020}
\end{figure}

```{r, echo=FALSE}
library(tidyverse)
library(survey)
```

\begin{figure}[H]
\centering
	\includegraphics[width=160mm]{images/enc.png}
	\caption{Pregunta de interés, demandas y necesidades de vivienda}
	\label{Pregunta de interés}
\end{figure}

```{r, echo=T, cache=TRUE}
# leemos la base de datos
setwd("D:/Notas/Muestreo/Exámen/examen02/Bases de datos")

thogar <- data.table::fread("THOGAR.csv")

# P3A1_1 es la variable a la que nos interesa replicar la estimación
# primero observamos que no hay N.A. por lo cual no es necesario hacer correciones
thogar2 <- thogar %>% drop_na(P3A1_1)
length(thogar2$P3A1_1) == length(thogar$P3A1_1)

# vemos las variables que usaremos en el diseño muestral
summary(thogar[, c("UPM_DIS", "EST_DIS", "FACTOR")])


#los que si tienen necesidad
# segun la estructura de archivo
# el número 1 corresponde a si tienen necesidad
a <- sum(thogar$P3A1_1 == 1)
# el número 2 a los que no
b <- sum(thogar$P3A1_1 == 2)
# el número 3 a los que no especificaron
c <- sum(thogar$P3A1_1 == 9)
# efectivamente estos son los únicos resultados
sum(a + b + c) == length(thogar$P3A1_1)

# añadimos estos valores al dataframe
# los que tienen necesida
thogar$si <- as.numeric(thogar$P3A1_1 == 1)
# los que no tienen necesidad
thogar$no <- as.numeric(thogar$P3A1_1 == 2)
# los que no especificaran
thogar$ne <- as.numeric(thogar$P3A1_1 == 9)
# ocuparemos un vector de unos para calcular el total
thogar$total <- 1

library(survey)
# en muchos casos sólo hay una upm en cada estrato, lo que 
# dificulta la estimación de la varianza por esto ocupamos esta opción
options(survey.lonely.psu="adjust")
# usamos nest=TRUE ya que no hay seguridad de que las claves de las UPM son únicas

# definimos el diseño
dsg.envi <- svydesign(id=~UPM_DIS, strat=~EST_DIS, weight =~FACTOR,
                      data = thogar, nest=T)
# summary(dsg.envi)

# guardamos los resultados
# por nivel nacional
rel.nac <- svymean(~si + no + ne, dsg.envi)*100
abs.nac <- svytotal(~si + no + ne, dsg.envi)
total.nac <- svytotal(~total, dsg.envi)

# por entidadad 
rel.ent <- svyby(~si +no + ne,~ENT,design=dsg.envi, svymean)
abs.ent <- svyby(~si+ no + ne,~ENT,design=dsg.envi, svytotal)
total.ent <- svyby(~total, ~ENT,design=dsg.envi, svytotal)

#anexar los nombres de las entidades (para hacer las tablas)
entidades=c("AGU", "BCN", "BCS", "CAM", "COA", "COL", "CHP", "CHH", "CMX", 
            "DUR", "GUA", "GRO", "HID", "JAL", "MEX", "MIC", "MOR", "NAY", "NLE",
            "OAX", "PUE", "QUE", "ROO", "SLP", "SIN", "SON", "TAB", "TAM", "TLA",
            "VER", "YUC", "ZAC")

#mostramos los resultados
```
```{r, echo=T}
total.nacdf <- as.data.frame(total.nac)
colnames(total.nacdf) <- c("total", "se")
row.names(total.nacdf) <- c("Nivel nacional")

total.entdf <- total.ent[, c("total", "se")]
row.names(total.entdf) <- entidades

dftotal <- union_all(total.nacdf, total.entdf)
colnames(dftotal) <- c("Total", "Error estándar")

kbl(dftotal, caption = "Total de hogares", booktabs = T) %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))
```


```{r, echo=T}
abs.nacdf <- as.data.frame(abs.nac)
colnames(abs.nacdf) <- c("Absoluto", "Error estándar de absoluto")
rel.nacdf <- as.data.frame(rel.nac)
colnames(rel.nacdf) <- c("Relativo", "Error estándar de relativo")


n.nac <- cbind(abs.nacdf, rel.nacdf)
rownames(n.nac) <- c("Si", "No", "No sabe")
  
kbl(n.nac, 
    caption = "Condición de hogares con necesidad de vivienda, nivel nacional", 
    booktabs = T) %>% kable_styling(latex_options = c("striped", "HOLD_position"))
```
SE significará Error estándar.
```{r, echo=T}
abs.entdf <- as.data.frame(abs.ent)
abs.entdf <- abs.entdf[c(-1)]
row.names(abs.entdf) <- entidades
colnames(abs.entdf) <- c("Si", "NO", "No sabe", "SE-Si", "SE-No", 
                         "SE-No sabe")

kbl(abs.entdf, 
    caption = "Condición de hogares con necesidad de vivienda, absoluto nivel entidad", 
    booktabs = T) %>% kable_styling(latex_options = c("striped", "HOLD_position"))
```
```{r, echo=T}
rel.entdf <- as.data.frame(rel.ent)
rel.entdf <- rel.entdf[c(-1)]
rel.entdf <- rel.entdf*100
row.names(rel.entdf) <- entidades
colnames(rel.entdf) <- c("Si", "NO", "No sabe", "SE-Si", "SE-No", "SE-No sabe")

kbl(rel.entdf, 
    caption = "Condición de hogares con necesidad de vivienda, 
    relativo nivel entidad", 
    booktabs = T) %>% kable_styling(latex_options = c("striped", "HOLD_position"))
```
```{r, echo=T}
total.nac.ic <- as.data.frame(confint(total.nac)) 
rownames(total.nac.ic) <- "Nivel nacional"
total.ent.ic <- as.data.frame(confint(total.ent))
rownames(total.ent.ic) <- entidades

total.ic <- union_all(total.nac.ic, total.ent.ic)

kbl(total.ic, 
    caption = "Intervalos confianza, total,
    condición de hogares con necesidad de vivienda", 
    booktabs = T) %>% kable_styling(latex_options = c("striped", "HOLD_position"))
```
Ahora presentamos los intervalos de conianza, suponiendo que el estimador sigue una distribución Normal.
```{r,echo=T}
abs.nac.ic <- as.data.frame(confint(abs.nac))
rownames(abs.nac.ic) <- c("Si", "No", "No sabe")

kbl(abs.nac.ic, 
    caption = "Intervalos confianza, absoluto nacional, 
    condición de hogares con necesidad de vivienda", 
    booktabs = T) %>% kable_styling(latex_options = c("striped", "HOLD_position"))
```

```{r, echo=T}
rel.nac.ic <- as.data.frame(confint(rel.nac))
rownames(rel.nac.ic) <- c("Si", "No", "No sabe")

kbl(rel.nac.ic, 
    caption = "Intervalos confianza, relativo nacional, 
    condición de hogares con necesidad de vivienda", 
    booktabs = T) %>% kable_styling(latex_options = c("striped", "HOLD_position"))
```
```{r, echo=T}
abs.ent.ic <- as.data.frame(confint(abs.ent))
abs.ent.ic.si <- abs.ent.ic[1:32, ]
row.names(abs.ent.ic.si) <- entidades
kbl(abs.ent.ic.si, 
    caption = "Intervalos confianza, Sí, absoluto entidad, 
    condición de hogares con necesidad de vivienda", 
    booktabs = T) %>% kable_styling(latex_options = c("striped", "HOLD_position"))

abs.ent.ic.no <- abs.ent.ic[33:64, ]
row.names(abs.ent.ic.no) <- entidades
kbl(abs.ent.ic.no, 
    caption = "Intervalos confianza, No, absoluto entidad, 
    condición de hogares con necesidad de vivienda", 
    booktabs = T) %>% kable_styling(latex_options = c("striped", "HOLD_position"))

abs.ent.ic.ne <- abs.ent.ic[65:96, ]
row.names(abs.ent.ic.ne) <- entidades
# esto es ya que no hay números negativos
abs.ent.ic.ne$`2.5 %` <- pmax(0, abs.ent.ic.ne$`2.5 %`)
kbl(abs.ent.ic.ne, 
    caption = "Intervalos confianza, No sabe, absoluto entidad,
    condición de hogares con necesidad de vivienda", 
    booktabs = T) %>% kable_styling(latex_options = c("striped", "HOLD_position"))
```

```{r, echo=T}
rel.ent.ic <- as.data.frame(confint(rel.ent)*100)
rel.ent.ic.si <- rel.ent.ic[1:32, ]
row.names(rel.ent.ic.si) <- entidades
kbl(rel.ent.ic.si, 
    caption = "Intervalos confianza, Sí, relativo entidad, 
    condición de hogares con necesidad de vivienda", 
    booktabs = T) %>% kable_styling(latex_options = c("striped", "HOLD_position"))

rel.ent.ic.no <- rel.ent.ic[33:64, ]
row.names(rel.ent.ic.no) <- entidades
kbl(rel.ent.ic.no, 
    caption = "Intervalos confianza, No, relativo entidad,
    condición de hogares con necesidad de vivienda", 
    booktabs = T) %>% kable_styling(latex_options = c("striped", "HOLD_position"))

rel.ent.ic.ne <- rel.ent.ic[65:96, ]
row.names(rel.ent.ic.ne) <- entidades
# esto es ya que no hay números negativos
rel.ent.ic.ne$`2.5 %` <- pmax(0, rel.ent.ic.ne$`2.5 %`)
kbl(rel.ent.ic.ne, 
    caption = "Intervalos confianza, No sabe, relativo entidad,
    condición de hogares con necesidad de vivienda", 
    booktabs = T) %>% kable_styling(latex_options = c("striped", "HOLD_position"))
```


